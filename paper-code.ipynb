{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_miyhBzeOPUW",
        "outputId": "97038852-f617-49be-8738-5b3369a58c94",
        "collapsed": true
      },
      "source": [
        "!pip3 install transformers numpy torch sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 59.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 55.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 70.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbE-Aq89yS6m",
        "outputId": "6dd2d63f-c58e-4cd1-d853-32cc596d4388"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E8yVNVOh-qH"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas.core.base import NoNewAttributesMixin\n",
        "\n",
        "class DataLoader():\n",
        "\n",
        "    def __init__(self, train_path, valid_path = None, test_path = None):\n",
        "        self.train_path = train_path\n",
        "        self.valid_path = valid_path\n",
        "        self.test_path = test_path\n",
        "\n",
        "    def LoadData(self):\n",
        "        self.train_raw = pd.read_csv(self.train_path, encoding='latin-1')\n",
        "\n",
        "        if self.valid_path != None:\n",
        "            self.valid = pd.read_csv(self.valid_path, encoding='latin-1')\n",
        "\n",
        "        if self.test_path != None:\n",
        "            self.test = pd.read_csv(self.test_path, encoding='latin-1')\n",
        "\n",
        "        return self.train_raw, self.valid, self.test\n",
        "\n",
        "    def PrintDataShapes(self):\n",
        "        print(\"Shape of training data:\", self.train_raw.shape)\n",
        "        print(\"Shape of validaton data:\", self.valid.shape)\n",
        "        print(\"Shape of test data:\", self.test.shape)\n",
        "\n",
        "    def GetDFListsPerClass(self):\n",
        "        self.train_dfs = [self.train_raw.loc[self.train_raw['class']==val, :] for val in self.train_raw['class'].unique()]\n",
        "        self.valid_dfs = [self.valid.loc[self.valid['class']==val, :] for val in self.valid['class'].unique()]\n",
        "        self.test_dfs = [self.test.loc[self.test['class']==val, :] for val in self.test['class'].unique()]\n",
        "\n",
        "        return self.train_dfs, self.valid_dfs, self.test_dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV9GjUwaiJzo"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "from pandas.core.base import NoNewAttributesMixin\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "\n",
        "class IncrementalDataOrganizer():\n",
        "\n",
        "    def __init__(self, train, valid, test, train_dfs, valid_dfs, test_dfs, train_strategy = \"random\"):\n",
        "\n",
        "        self.train = train\n",
        "        self.valid = valid\n",
        "        self.test = test\n",
        "\n",
        "        self.train_dfs = train_dfs\n",
        "        self.valid_dfs = valid_dfs\n",
        "        self.test_dfs = test_dfs\n",
        "        self.train_strategy = train_strategy\n",
        "\n",
        "        self.all_classes = self.train[\"class\"].unique().tolist()\n",
        "\n",
        "        if self.train_strategy == \"random\":\n",
        "            self.remaining_classes = self.all_classes\n",
        "\n",
        "        if self.train_strategy == \"large_first\":\n",
        "            self.remaining_classes = self.train[\"class\"].value_counts().index.tolist()\n",
        "\n",
        "        if self.train_strategy == \"small_first\":\n",
        "            self.remaining_classes = self.train[\"class\"].value_counts().index.tolist()\n",
        "            self.remaining_classes = self.remaining_classes[::-1]\n",
        "\n",
        "        print(\"=============== All Class List ===============\")\n",
        "        print(self.remaining_classes)\n",
        "        print(\"=============== All Class List ===============\")\n",
        "\n",
        "        self.classes_already_in_training = []\n",
        "        self.record_indexes_already_in_training = []\n",
        "        self.weak_classes = []\n",
        "        self.strong_classes = []\n",
        "        self.normal_classes = []\n",
        "\n",
        "\n",
        "    def select_new_training_classes(self, num_classes):\n",
        "\n",
        "\n",
        "        self.selected_classes = []\n",
        "\n",
        "        if len(self.remaining_classes) < num_classes:\n",
        "\n",
        "            self.num_classes = len(self.remaining_classes)\n",
        "            self.selected_classes =  self.remaining_classes\n",
        "            self.remaining_classes = []\n",
        "\n",
        "        else:\n",
        "            if self.train_strategy == \"random\":\n",
        "                self.selected_classes = random.sample(self.remaining_classes, num_classes)\n",
        "\n",
        "            if  self.train_strategy == \"large_first\":\n",
        "                self.selected_classes = self.remaining_classes[:num_classes]\n",
        "\n",
        "            if self.train_strategy == \"small_first\":\n",
        "                self.selected_classes  = self.remaining_classes[:num_classes]\n",
        "\n",
        "\n",
        "        self.filtered_dfs = [df for df in self.train_dfs if df['class'].iloc[0] in self.selected_classes]\n",
        "\n",
        "        self.all_record_df = pd.concat(self.filtered_dfs)\n",
        "\n",
        "        #if first_selection != True:\n",
        "        self.classes_already_in_training = self.classes_already_in_training + self.selected_classes\n",
        "\n",
        "        all_record_df = shuffle(self.all_record_df)\n",
        "        #recordindexes_already_in_training =  recordindexes_already_in_training + all_record_df.index.tolist()\n",
        "\n",
        "        self.diff= list(set(self.all_classes) - set(self.classes_already_in_training))\n",
        "\n",
        "        self.remaining_classes = [o for o in self.remaining_classes if o in self.diff]\n",
        "\n",
        "        return self.selected_classes, self.filtered_dfs, self.all_record_df\n",
        "\n",
        "\n",
        "    def select_old_training_classes(self,class_list, num_prev_records, ratio = None):\n",
        "\n",
        "\n",
        "        self.old_filtered_dfs = [df for df in self.train_dfs if df['class'].iloc[0] in class_list]\n",
        "\n",
        "        self.partial_dfs = []\n",
        "        for df in self.old_filtered_dfs:\n",
        "            self.partial_dfs.append(self.get_partial_data(df,num_prev_records))\n",
        "\n",
        "        self.all__old_record_df = pd.concat(self.partial_dfs)\n",
        "\n",
        "        self.all__old_record_df = shuffle(self.all__old_record_df)\n",
        "        #recordindexes_already_in_training =  recordindexes_already_in_training + all_record_df.index.tolist()\n",
        "\n",
        "        return self.old_filtered_dfs, self.all__old_record_df\n",
        "\n",
        "\n",
        "    def select_validation_classes(self, records = None, ratio = None):\n",
        "\n",
        "        self.all_valid_classes = self.valid[\"class\"].unique().tolist()\n",
        "\n",
        "        self.selected_val_classes  = list(set(self.all_valid_classes) & set(self.classes_already_in_training))\n",
        "\n",
        "        self.filtered__val_dfs = [df for df in self.valid_dfs if df['class'].iloc[0] in self.selected_val_classes]\n",
        "\n",
        "        self.all_val_record_df = pd.concat(self.filtered__val_dfs)\n",
        "\n",
        "        self.all_val_record_df = shuffle(self.all_val_record_df)\n",
        "\n",
        "        return self.selected_val_classes, self.filtered__val_dfs, self.all_val_record_df\n",
        "\n",
        "    def get_partial_data(self, dataset, num_prev_records):\n",
        "        #partial_dataset = dataset.sample(frac = ratio)\n",
        "        if len(dataset) < num_prev_records:\n",
        "            self.partial_dataset = dataset\n",
        "        elif len(dataset) >= num_prev_records:\n",
        "            self.partial_dataset = dataset.sample(n = num_prev_records)\n",
        "\n",
        "        return self.partial_dataset\n",
        "\n",
        "\n",
        "\n",
        "    def GetDataBatches(self, num_classes, num_prev_records, num_base_classes):\n",
        "\n",
        "        self.number_of_classes = num_classes\n",
        "        self.num_base_classes = num_base_classes\n",
        "\n",
        "        base_classes, base_dfs, base_records = self.select_new_training_classes(self.num_base_classes)\n",
        "        base_classes_val, base_dfs_val, base_records_val = self.select_validation_classes()\n",
        "\n",
        "        self.training_dfs = [base_records]\n",
        "        self.Validation_dfs = [base_records_val]\n",
        "\n",
        "        num_batches = math.ceil((47-num_classes)/num_classes)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            if len(self.remaining_classes) == 0:\n",
        "                break\n",
        "            self.new_selected_classes, self.filtered_train_dfs, self.all_filt_train_records_df = self.select_new_training_classes(num_classes)\n",
        "            self.old_classes = list(set(self.classes_already_in_training) - set(self.new_selected_classes))\n",
        "            self.filtered_old_train_dfs, self.all_old_filt_train_records_df = self.select_old_training_classes(self.old_classes, num_prev_records)\n",
        "\n",
        "            self.all_filt_train_records_df = self.all_filt_train_records_df.append(self.all_old_filt_train_records_df)\n",
        "\n",
        "            self.all_filt_train_records_df = shuffle(self.all_filt_train_records_df)\n",
        "\n",
        "            self.training_dfs.append(self.all_filt_train_records_df)\n",
        "\n",
        "            self.classes_for_validation = self.new_selected_classes + self.old_classes\n",
        "            self.base_classes_val, self.base_dfs_val, self.base_records_val = self.select_validation_classes(self.classes_for_validation)\n",
        "            self.Validation_dfs.append(self.base_records_val)\n",
        "\n",
        "            print(\"New Training Classes:\" + str(self.new_selected_classes))\n",
        "            #print(\"Old Classes in Training Set:\" + str(old_classes))\n",
        "            #print(\"Number of Old Classes in Training Set:\" + str(len(self.old_classes)))\n",
        "\n",
        "            print(\"Unique Classes in New train  Dataframe:\" + str(self.all_filt_train_records_df[\"class\"].nunique()))\n",
        "            print(\"Unique Classes in New val Dataframe:\" + str(self.base_records_val[\"class\"].nunique()))\n",
        "\n",
        "            #print(\"Shape of old dataframe:\" + str(all_old_filt_train_records_df.shape))\n",
        "            print(\"Remaining classes Classes:\" + str(self.remaining_classes))\n",
        "            print(\"Number of remaining classes:\" + str(len(self.remaining_classes)))\n",
        "\n",
        "            # print(\"===========================================================================\")\n",
        "\n",
        "        return self.training_dfs, self.Validation_dfs\n",
        "\n",
        "\n",
        "    def GetSampleData(self, training_dfs, validation_dfs, num_batches, instances_per_batch):\n",
        "\n",
        "        training_dfs_short = training_dfs[:num_batches]\n",
        "        validation_dfs_short = validation_dfs[:num_batches]\n",
        "\n",
        "        self.training_dfs_short = [df.head(n = instances_per_batch) for df in training_dfs_short]\n",
        "        self.validation_dfs_short = [df.head(n = instances_per_batch) for df in validation_dfs_short]\n",
        "\n",
        "        return self.training_dfs_short, self.validation_dfs_short\n",
        "\n",
        "\n",
        "    def GetTrainingBatchesOnly(self, num_classes):\n",
        "\n",
        "            self.number_of_classes = num_classes\n",
        "            base_classes, base_dfs, base_records = self.select_new_training_classes(self.number_of_classes)\n",
        "            base_classes_val, base_dfs_val, base_records_val = self.select_validation_classes()\n",
        "\n",
        "            self.training_dfs = [base_records]\n",
        "            self.Validation_dfs = [base_records_val]\n",
        "\n",
        "\n",
        "            num_batches = math.ceil((47-num_classes)/num_classes)\n",
        "\n",
        "            self.previous_classes_list = [[]]\n",
        "\n",
        "            print(\"Number of Unique Classes in New train  Dataframe:\" + str(base_records[\"class\"].nunique()))\n",
        "            print(\"Unique Classes in New train  Dataframe:\" + str(base_records[\"class\"].unique()))\n",
        "\n",
        "            print(\"Number of Unique Classes in New val Dataframe:\" + str(base_records_val[\"class\"].nunique()))\n",
        "            print(\"Unique Classes in New val Dataframe:\" + str(base_records_val[\"class\"].unique()))\n",
        "\n",
        "            print(\"Old Training Classes:\" + \"Nill\")\n",
        "\n",
        "            print(\"===========================================================================\")\n",
        "\n",
        "            for i in range(num_batches):\n",
        "                if len(self.remaining_classes) == 0:\n",
        "                    break\n",
        "\n",
        "                self.new_selected_classes, self.filtered_train_dfs, self.all_filt_train_records_df = self.select_new_training_classes(num_classes)\n",
        "                self.training_dfs.append(self.all_filt_train_records_df)\n",
        "\n",
        "                self.all_training_records_df = pd.concat(self.training_dfs)\n",
        "\n",
        "                self.classes_for_validation = self.all_training_records_df['class'].unique()\n",
        "                self.base_classes_val, self.base_dfs_val, self.base_records_val = self.select_validation_classes(self.classes_for_validation)\n",
        "                self.Validation_dfs.append(self.base_records_val)\n",
        "\n",
        "\n",
        "                print(\"Number of Unique Classes in New train  Dataframe:\" + str(self.all_filt_train_records_df[\"class\"].nunique()))\n",
        "                print(\"Unique Classes in New train  Dataframe:\" + str(self.all_filt_train_records_df[\"class\"].unique()))\n",
        "\n",
        "                print(\"Number of Unique Classes in New val Dataframe:\" + str(self.base_records_val[\"class\"].nunique()))\n",
        "                print(\"Unique Classes in New val Dataframe:\" + str(self.base_records_val[\"class\"].unique()))\n",
        "\n",
        "\n",
        "                self.old_classes_only = list(set(self.all_training_records_df['class'].unique()) - set (self.all_filt_train_records_df[\"class\"].unique()))\n",
        "\n",
        "                self.previous_classes_list.append(self.old_classes_only)\n",
        "                print(\"Old Training Classes:\" + str(self.previous_classes_list[i+1]))\n",
        "\n",
        "                print(\"===========================================================================\")\n",
        "\n",
        "            return self.training_dfs, self.Validation_dfs, self.previous_classes_list\n",
        "\n",
        "\n",
        "\n",
        "    def GetFixedDataBatches(self, training_dfs, previous_class_list, num_prev_records):\n",
        "\n",
        "          self.training_dfs = []\n",
        "\n",
        "          i = 0\n",
        "          for tdf, prev_class_list in zip(training_dfs, previous_class_list):\n",
        "\n",
        "              shape = tdf.shape\n",
        "\n",
        "              if i > 0:\n",
        "                  self.filtered_old_train_dfs, self.all_old_filt_train_records_df = self.select_old_training_classes(prev_class_list, num_prev_records)\n",
        "                  self.all_filt_train_records_df = tdf.append(self.all_old_filt_train_records_df)\n",
        "\n",
        "\n",
        "                  self.all_filt_train_records_df = shuffle(self.all_filt_train_records_df)\n",
        "\n",
        "                  shape2 = self.all_filt_train_records_df.shape\n",
        "\n",
        "                  self.training_dfs.append(self.all_filt_train_records_df)\n",
        "\n",
        "                  print(\"*******************************\")\n",
        "                  print(\"*******************************\")\n",
        "                  print(\"*******************************\")\n",
        "                  print(\"*******************************\")\n",
        "\n",
        "                  print(\"Number of Unique Classes in New train  Dataframe:\" + str(tdf[\"class\"].nunique()))\n",
        "                  print(\"Unique Classes in New train  Dataframe:\" + str(tdf[\"class\"].unique()))\n",
        "\n",
        "                  print(\"Number of Unique Classes in old dataframe:\" + str(self.all_old_filt_train_records_df[\"class\"].nunique()))\n",
        "                  print(\"Unique Classes in old dataframe:\" + str(self.all_old_filt_train_records_df[\"class\"].unique()))\n",
        "\n",
        "              else:\n",
        "                  self.training_dfs.append(tdf)\n",
        "\n",
        "              i = i + 1\n",
        "\n",
        "\n",
        "\n",
        "          return self.training_dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6-zeGCO_mm"
      },
      "source": [
        "import torch\n",
        "from transformers import FlaubertTokenizer, FlaubertModel, FlaubertConfig, FlaubertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class FlaubertDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts_list = dataset[\"text\"].tolist()\n",
        "        self.encodings = tokenizer(self.texts_list, truncation=True, padding=True, max_length=512)\n",
        "        self.labels = dataset[\"class\"].tolist()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "class ModelInitializer():\n",
        "\n",
        "    def __init__(self, max_length):\n",
        "\n",
        "        #os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.training_args = TrainingArguments(\n",
        "                                                report_to=None,\n",
        "                                                output_dir='./results',          # output directory\n",
        "                                                num_train_epochs = 5,              # total number of training epochs\n",
        "                                                per_device_train_batch_size=8,   # batch size per device during training\n",
        "                                                per_device_eval_batch_size=8,\n",
        "                                                learning_rate = 2e-5,            # batch size for evaluation\n",
        "                                                warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "                                                weight_decay=0.01,               # strength of weight decay\n",
        "                                                #logging_dir='./logs',           # directory for storing logs\n",
        "                                                load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "                                                # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "                                                #logging_steps=20000,               # log & save weights each logging_steps\n",
        "                                                #save_steps = 20000,\n",
        "                                                save_strategy = \"epoch\",\n",
        "                                                evaluation_strategy=\"epoch\",     # evaluate each `logging_steps`\n",
        "                                            )\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_metrics(pred):\n",
        "        labels = pred.label_ids\n",
        "        preds = pred.predictions.argmax(-1)\n",
        "        # calculate accuracy using sklearn's function\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        return {\n",
        "            'accuracy': acc,\n",
        "        }\n",
        "\n",
        "\n",
        "    def softmax(self, X, theta = 1.0, axis = None):\n",
        "\n",
        "        # make X at least 2d\n",
        "        y = np.atleast_2d(X)\n",
        "\n",
        "        # find axis\n",
        "        if axis is None:\n",
        "            axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
        "\n",
        "        # multiply y against the theta parameter,\n",
        "        y = y * float(theta)\n",
        "\n",
        "        # subtract the max for numerical stability\n",
        "        y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
        "\n",
        "        # exponentiate y\n",
        "        y = np.exp(y)\n",
        "\n",
        "        # take the sum along the specified axis\n",
        "        ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
        "\n",
        "        # finally: divide elementwise\n",
        "        p = y / ax_sum\n",
        "\n",
        "        # flatten if X was 1D\n",
        "        if len(X.shape) == 1: p = p.flatten()\n",
        "\n",
        "        return p\n",
        "\n",
        "    def StartTraining (self,  model_name, model_type, num_labels_p, training_dfs, validation_dfs, instances_from_old_classes):\n",
        "        predictions_array = []\n",
        "        labels = []\n",
        "\n",
        "        self.tokenizer = FlaubertTokenizer.from_pretrained(model_name)\n",
        "        self.model = FlaubertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels_p).to(\"cuda\")\n",
        "\n",
        "\n",
        "        print(\"==========================================\")\n",
        "        print(\"==========================================\")\n",
        "        print (\"Total Batches:\", len(training_dfs))\n",
        "        print(\"==========================================\")\n",
        "        print(\"==========================================\")\n",
        "\n",
        "        i = 0\n",
        "\n",
        "        for train, val in zip(training_dfs, validation_dfs):\n",
        "\n",
        "            print(\"==========================================\")\n",
        "            print(\"==========================================\")\n",
        "            print (\"Number of old Instances:\", (instances_from_old_classes))\n",
        "            print (\"Processing Batch Number:\", (i + 1))\n",
        "            print (\"Total Batches:\", len(training_dfs))\n",
        "            print(\"==========================================\")\n",
        "            print(\"==========================================\")\n",
        "\n",
        "            train_dataset = FlaubertDataset(train, self.tokenizer)\n",
        "            valid_dataset = FlaubertDataset(val, self.tokenizer)\n",
        "\n",
        "            trainer = Trainer(\n",
        "                model= self.model,                         # the instantiated Transformers model to be trained\n",
        "                args= self.training_args,                  # training arguments, defined above\n",
        "                train_dataset = train_dataset,         # training dataset\n",
        "                eval_dataset = valid_dataset,          # evaluation dataset\n",
        "                compute_metrics= ModelInitializer.compute_metrics,     # the callback that computes metrics of interest\n",
        "            )\n",
        "\n",
        "            trainer.train()\n",
        "\n",
        "            print(\"==============  training complete  ==================\")\n",
        "\n",
        "            pred = trainer.predict(valid_dataset)\n",
        "            predictions_array.append(pred)\n",
        "            labels.append(val[\"class\"].tolist())\n",
        "\n",
        "            print(\"==============  evaluation complete  ==================\")\n",
        "\n",
        "            predictions = self.softmax(pred.predictions, axis = 1)\n",
        "            y_pred = list((np.argmax(predictions,axis= 1)))\n",
        "\n",
        "            y_test = val[\"class\"].tolist()\n",
        "\n",
        "            from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "            print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "            i = i + 1\n",
        "\n",
        "            shutil.rmtree(\"/content/results\")\n",
        "\n",
        "            print(\"===========================================\")\n",
        "            print(\"Checkpoints deleted\")\n",
        "            print(\"===========================================\")\n",
        "\n",
        "\n",
        "        test_dataset = FlaubertDataset(test, self.tokenizer)\n",
        "        result = trainer.predict(test_dataset)\n",
        "\n",
        "        return predictions_array, labels, result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY0rAeSzPCr1"
      },
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "\n",
        "file_train_path = \"/content/drive/MyDrive/datasets/text_classification/data_train.csv\"\n",
        "file_test_path = \"/content/drive/MyDrive/datasets/text_classification/data_test.csv\"\n",
        "file_val_path = \"/content/drive/MyDrive/datasets/text_classification/data_valid.csv\"\n",
        "\n",
        "train_raw = pd.read_csv(file_train_path, encoding='latin-1')\n",
        "valid = pd.read_csv(file_val_path, encoding='latin-1')\n",
        "test = pd.read_csv(file_test_path, encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B5g06ZtI7eF"
      },
      "source": [
        "import torch\n",
        "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
        "from transformers import FlaubertTokenizer, FlaubertModel, FlaubertConfig, FlaubertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.utils import shuffle, validation\n",
        "import pdb\n",
        "import os\n",
        "\n",
        "\n",
        "file_train_path = \"/content/drive/MyDrive/datasets/text_classification/data_train.csv\"\n",
        "file_test_path = \"/content/drive/MyDrive/datasets/text_classification/data_test.csv\"\n",
        "file_val_path = \"/content/drive/MyDrive/datasets/text_classification/data_valid.csv\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxIwUUCJimZs",
        "outputId": "617c31d8-da2f-44ee-f79b-46e197e4c8f7"
      },
      "source": [
        "data_loader = DataLoader(file_train_path, file_val_path, file_test_path)\n",
        "\n",
        "train, valid, test = data_loader.LoadData()\n",
        "train_dfs, valid_dfs, test_dfs = data_loader.GetDFListsPerClass()\n",
        "\n",
        "\n",
        "print(\"************** document loaded ****************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************** document loaded ****************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJeo4Rja8GMp"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t4ihBiPlu_x"
      },
      "source": [
        "import pickle\n",
        "\n",
        "instance_list = [50]\n",
        "result_path = \"/content/drive/MyDrive/datasets/text_classification/Flaubert models/random_5_5_50_instances\"\n",
        "\n",
        "result_list = []\n",
        "\n",
        "for instances_from_old_class in instance_list:\n",
        "    print(\"==========================================\")\n",
        "    print(\"==========================================\")\n",
        "    print(\"==========================================\")\n",
        "    print (\"Instances from old class:\", instances_from_old_class)\n",
        "    print(\"==========================================\")\n",
        "    print(\"==========================================\")\n",
        "    print(\"==========================================\")\n",
        "\n",
        "\n",
        "    incr_data_organizer = IncrementalDataOrganizer(train,valid,test,train_dfs,valid_dfs,test_dfs, train_strategy= \"small_first\")\n",
        "\n",
        "\n",
        "    classes_per_batch = 5\n",
        "    num_base_classes = 5\n",
        "    #instances_from_old_classes = 100\n",
        "\n",
        "\n",
        "    training_dfs, validation_dfs = incr_data_organizer.GetDataBatches(classes_per_batch, instances_from_old_class, num_base_classes)\n",
        "\n",
        "\n",
        "\n",
        "    print(len(training_dfs))\n",
        "    print(training_dfs[0].shape)\n",
        "\n",
        "\n",
        "    model_name = 'flaubert/flaubert_base_cased'\n",
        "    num_labels = 47\n",
        "    max_length = 512\n",
        "\n",
        "    incremental_model = ModelInitializer( max_length)\n",
        "    predictions_array, labels, test_results = incremental_model.StartTraining(model_name, \"flaubert\", num_labels, training_dfs, validation_dfs, instances_from_old_class)\n",
        "\n",
        "    result = (instances_from_old_class, predictions_array, labels, test_results)\n",
        "    result_list.append(result)\n",
        "\n",
        "\n",
        "    outfile = open(result_path,'wb')\n",
        "    pickle.dump(result_list, outfile)\n",
        "    outfile.close()\n",
        "\n",
        "print(\"Total batches processed:\",len(result_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTuwGb1Bssu3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}